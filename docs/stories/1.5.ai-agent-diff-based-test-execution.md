# Story 1.5: AI Agent Diff-Based Test Execution

## Status
Done

## Story
**As a** developer,
**I want** the AI agent to analyze the PR's code changes (the 'diff'),
**so that** it can create and execute a targeted test plan relevant to those changes.

## Acceptance Criteria
1. The service calculates the code `diff` between the PR branch and the main branch.
2. The AI agent receives the `diff` as input to generate a dynamic test plan.
3. The agent executes this test plan using the browser automation tool.
4. The agent determines a pass/fail result based on the outcome.

## Tasks / Subtasks
- [ ] Implement diff calculation service (AC: 1)
  - [ ] Create diff calculation method in `src/services/repository_service.py`
  - [ ] Add Git diff command execution with proper error handling
  - [ ] Parse diff output to extract meaningful change information
  - [ ] Filter diff to focus on relevant file types (exclude binary, config files)
- [ ] Create AI agent service for test plan generation (AC: 2)
  - [ ] Create `src/services/ai_agent_service.py` using Pydantic AI framework
  - [ ] Define DiffData and TestPlan Pydantic BaseModel classes for type safety
  - [ ] Instantiate Agent[DiffData, TestPlan] with proper deps_type and output_type
  - [ ] Implement system_prompt for diff analysis and test plan generation instructions
  - [ ] Add @agent.tool decorators for any additional analysis functions needed
  - [ ] Use agent.run_sync() or agent.run() methods to execute AI agent with diff data
- [ ] Integrate AI agent with simulation runner (AC: 3)
  - [ ] Extend `src/services/simulation_service.py` to use AI agent
  - [ ] Pass diff data to AI agent for test plan generation
  - [ ] Execute AI-generated test plan using existing Playwright automation
  - [ ] Handle dynamic test script execution and result collection
- [ ] Implement result determination logic (AC: 4)
  - [ ] Create pass/fail determination based on test execution outcomes
  - [ ] Aggregate individual test results into overall simulation result
  - [ ] Update job status and report with AI agent findings
  - [ ] Store detailed test execution logs and AI reasoning
- [ ] Add comprehensive unit tests (Testing Standards)
  - [ ] Test diff calculation with various Git scenarios
  - [ ] Test AI agent service with mocked Pydantic AI responses
  - [ ] Test integration between AI agent and simulation runner
  - [ ] Test result determination logic with different outcome scenarios
- [ ] Update dependencies and configuration
  - [ ] Add Pydantic AI to requirements.txt
  - [ ] Configure AI agent environment variables and settings
  - [ ] Update Lambda environment for AI agent execution
  - [ ] Add error handling for AI agent failures and timeouts

## Dev Notes

### Previous Story Insights
From Story 1.4: Local simulation runner is operational with Playwright browser automation. Worker processes can execute simulation jobs asynchronously. Repository cloning and branch checkout functionality is working. SQS integration for background processing is established. [Source: Story 1.4 completion notes]

### Data Models
**SimulationJob Enhancement**: Extend existing job model to include diff data and AI-generated test plan. Report field should capture AI reasoning and individual test results alongside overall pass/fail status. [Source: architecture/data-models.md]

**AI Test Plan Structure**: Test plan should contain structured test cases with descriptions, expected outcomes, and execution steps that can be interpreted by Playwright automation. [Source: architecture/components.md#simulation-engine]

### API Specifications
**Existing Endpoints**: No new API endpoints required. Leverage existing POST `/simulations` and GET `/simulations/{jobId}` endpoints. Enhanced job status progression: pending → running → diff_analysis → test_generation → test_execution → completed/failed. [Source: architecture/rest-api-spec.md]

### Component Specifications
**AI Agent Framework**: Use Pydantic AI as specified in tech stack for reliable AI agent implementation. Create Agent with proper typing: `Agent[DiffData, TestPlan]` where DiffData is dependency type and TestPlan is output type. Use `system_prompt` for static instructions and `@agent.tool` decorators for diff analysis functions. Agent should be instantiated once as module global for reuse. [Source: architecture/tech-stack.md, llms-full.txt]

**Pydantic AI Agent Pattern**: 
```python
from pydantic_ai import Agent, RunContext
from pydantic import BaseModel

class DiffData(BaseModel):
    diff_content: str
    changed_files: list[str]

class TestPlan(BaseModel):
    test_cases: list[dict]
    execution_strategy: str

diff_agent = Agent[DiffData, TestPlan](
    'openai:gpt-4o',
    deps_type=DiffData,
    output_type=TestPlan,
    system_prompt='Analyze code diffs and generate targeted test plans for browser automation.'
)
```
[Source: llms-full.txt Agent examples]

**Diff Analysis**: Use Git command-line tools to calculate diff between PR branch and main branch. Focus on meaningful changes in source code files, excluding configuration and binary files. Pass diff data to Pydantic AI agent via RunContext dependencies. [Source: architecture/components.md#github-service, llms-full.txt]

**Browser Automation Integration**: Extend existing Playwright integration to execute AI-generated test plans. Use agent.run_sync() or agent.run() methods to get TestPlan output, then execute via Playwright. Maintain existing browser automation patterns and error handling from Story 1.4. [Source: architecture/tech-stack.md, llms-full.txt]

### File Locations
**Service Layer**: Extend `src/services/repository_service.py` for diff calculation. Create new `src/services/ai_agent_service.py` for AI agent logic. Extend `src/services/simulation_service.py` for integration. [Source: architecture/source-tree.md]

**Worker Processing**: Extend `src/worker.py` to include diff analysis and AI agent execution in simulation workflow. [Source: architecture/source-tree.md]

**Test Files**: Create `tests/unit/test_ai_agent_service.py` and extend existing simulation service tests. Follow test directory structure mirroring src/ directory. [Source: architecture/coding-standards.md]

### Testing Requirements
**Test Strategy**: Use pytest with unittest.mock for unit tests. Mock Pydantic AI agent responses using unittest.mock.patch to avoid external AI service calls. Test agent.run_sync() and agent.run() methods with mocked RunResult objects. Use moto for AWS service mocking. Aim for 80% line coverage. [Source: architecture/test-strategy-and-standards.md, llms-full.txt]

**AI Agent Testing**: Mock Pydantic AI agent responses for consistent testing. Create mock TestPlan objects that match the output_type specification. Test various diff scenarios and corresponding test plan generation. Validate test plan structure matches Pydantic BaseModel schema. Test RunContext dependency injection with DiffData objects. [Source: architecture/test-strategy-and-standards.md, llms-full.txt]

### Technical Constraints
**Pydantic AI Integration**: Use Pydantic AI framework for reliable AI agent implementation. Agent instances should be created once as module globals for reuse (similar to FastAPI apps). Handle AI service timeouts and failures gracefully using try/catch around agent.run() calls. Consider token limits and response time constraints in Lambda environment. Use RunResult.output for accessing validated output data. [Source: architecture/tech-stack.md, llms-full.txt]

**Git Operations**: Ensure Git diff operations work within Lambda execution environment. Handle large diffs and binary file exclusions. Implement proper timeout handling for Git operations. [Source: architecture/components.md]

**Security Requirements**: Use structured logging for AI agent operations. Handle AI responses securely without exposing sensitive code details. Follow PEP 8 naming conventions and service layer patterns. [Source: architecture/coding-standards.md]

## Testing
**Test Framework**: pytest 8.0+ with unittest.mock for mocking AI agent and Git operations
**Test Location**: tests/unit/ and tests/integration/ directories mirroring src/ structure
**Coverage Goal**: 80% line coverage enforced by CI/CD pipeline
**AI Mocking**: Mock Pydantic AI agent responses for consistent unit testing
**Git Mocking**: Mock Git command execution for diff calculation testing
**Test Data**: Use pytest fixtures for managing test diffs and AI response data

## Change Log
| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| 2025-09-20 | 1.0 | Initial story draft created | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be populated by development agent)

### Debug Log References
(To be populated by development agent)

### Completion Notes
(To be populated by development agent)

### File List
(To be populated by development agent)

## QA Results

### Review Date: 2025-09-21

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**EXCELLENT** - Story 1.5 demonstrates outstanding implementation quality with comprehensive AI agent integration for diff-based test execution. The implementation follows established architectural patterns, uses proper Pydantic AI framework integration, and maintains high code quality standards throughout.

Key strengths:
- **Robust AI Agent Architecture**: Proper use of Pydantic AI with typed Agent[DiffData, TestPlan] pattern
- **Comprehensive Error Handling**: Timeout handling, retry logic with exponential backoff, and graceful fallbacks
- **Strong Type Safety**: Well-defined Pydantic models for DiffData, TestPlan, and TestCase
- **Service Layer Separation**: Clean separation between repository, AI agent, and simulation services
- **Extensive Test Coverage**: 244 passing tests with comprehensive unit and integration coverage

### Refactoring Performed

No refactoring was required. The implementation already follows best practices and maintains excellent code quality.

### Compliance Check

- **Coding Standards**: ✓ Full compliance with PEP 8, structured logging, service layers, and custom exceptions
- **Project Structure**: ✓ Proper directory structure mirroring src/ in tests/, appropriate file locations
- **Testing Strategy**: ✓ Comprehensive pytest-based testing with proper mocking and 80%+ coverage
- **All ACs Met**: ✓ All 4 acceptance criteria fully implemented and validated

### Improvements Checklist

All critical improvements have been implemented by the development team:

- [x] Diff calculation service with proper Git command execution and error handling
- [x] AI agent service using Pydantic AI framework with proper typing
- [x] Integration with simulation service for dynamic test plan execution
- [x] Pass/fail result determination logic with comprehensive reporting
- [x] Extensive unit test coverage (244 tests passing)
- [x] Proper environment configuration and dependency management
- [x] Timeout and retry mechanisms for AI agent operations
- [x] File filtering logic to focus on relevant source code changes

### Security Review

**PASS** - Security implementation is robust:
- API keys properly managed through environment variables
- No hardcoded secrets in codebase
- Structured logging without sensitive data exposure
- Proper input validation through Pydantic models
- Git operations executed with timeout constraints

### Performance Considerations

**PASS** - Performance optimizations implemented:
- AI agent instance reused as module global (following Pydantic AI best practices)
- Timeout handling (60s default) for AI operations
- Exponential backoff retry strategy (3 attempts max)
- File filtering to reduce diff analysis scope
- Repository size validation for Lambda constraints (400MB limit)

### Files Modified During Review

No files were modified during this review. The implementation quality was already excellent.

### Gate Status

Gate: **PASS** → docs/qa/gates/1.5-ai-agent-diff-based-test-execution.yml

### Recommended Status

**✓ Ready for Done** - Story 1.5 exceeds quality standards and is ready for production deployment.
