# Story 1.4: Implement Local Simulation Runner

## Status
Approved

## Story
**As a** reviewer,
**I want** to trigger a simulation process,
**so that** the checked-out PR code is executed in a controlled local environment.

## Acceptance Criteria
1. A new endpoint exists to start the simulation for a previously fetched PR.
2. The service successfully launches the browser automation tool from the root of the local repository clone, which is now on the correct PR branch.
3. The runner can execute a test script generated by analyzing the PR diff.
4. The process runs asynchronously in the background.

## Tasks / Subtasks
- [ ] Create simulation runner service component (AC: 1, 2, 3, 4)
  - [ ] Create SimulationRunner service class in `src/services/simulation_service.py`
  - [ ] Implement method to launch Playwright browser automation from repository root
  - [ ] Add configuration for browser automation environment setup
  - [ ] Implement diff-based test script generation and execution logic
- [ ] Implement simulation start endpoint (AC: 1, 4)
  - [ ] Create POST `/simulations/{job_id}/start` endpoint in `src/api/simulations.py`
  - [ ] Add authentication middleware protection
  - [ ] Validate job exists and is in correct state for simulation
  - [ ] Queue simulation job to SQS for asynchronous processing
  - [ ] Return immediate response with updated job status
- [ ] Extend worker.py for simulation processing (AC: 2, 3, 4)
  - [ ] Add simulation job handler in `src/worker.py`
  - [ ] Integrate with repository service to ensure correct branch checkout
  - [ ] Launch browser automation from repository root directory
  - [ ] Generate test script from PR diff and execute using Playwright
  - [ ] Update job status in DynamoDB throughout process
  - [ ] Handle simulation errors and timeouts gracefully
- [ ] Add comprehensive unit tests (Testing Standards)
  - [ ] Test simulation runner service with mocked Playwright
  - [ ] Test simulation start endpoint with valid/invalid job IDs
  - [ ] Test worker simulation processing with mocked browser automation
  - [ ] Test error handling for browser launch failures
  - [ ] Test asynchronous job status updates
- [ ] Update infrastructure configuration
  - [ ] Add Playwright dependencies to requirements.txt
  - [ ] Configure Lambda environment for browser automation (if feasible)
  - [ ] Update IAM policies for SQS queue access
  - [ ] Add environment variables for simulation configuration

## Dev Notes

### Previous Story Insights
From Story 1.3: Repository cloning and branch checkout functionality is fully operational. GitHub service can fetch PR data and metadata. DynamoDB job tracking system is working with proper status management. Local repository clones are persisted and ready for simulation execution. [Source: Story 1.3 completion notes]

### Data Models
**SimulationJob Status Updates**: Extend existing status enum to include 'simulation_running' and 'simulation_completed' states. Job model already supports report field for storing simulation results. [Source: architecture/data-models.md]

**SimulationResult Structure**: Report object should contain result (pass/fail), summary text, and execution logs as defined in REST API spec. [Source: architecture/rest-api-spec.md#SimulationJob]

### API Specifications
**New Endpoint**: POST `/simulations/{jobId}/start` - Start simulation for existing job. Requires authentication via JWT bearer token. Returns 202 Accepted with updated job status. [Source: architecture/rest-api-spec.md]

**Existing Endpoints**: Leverage existing GET `/simulations/{jobId}` for status checking. Job status progression: pending → running → simulation_running → simulation_completed/failed. [Source: architecture/rest-api-spec.md]

### Component Specifications
**Browser Automation**: Use Playwright 1.40+ as specified in tech stack. Launch headless browser from repository root directory. Generate and execute test scripts based on PR diff analysis in controlled environment. [Source: architecture/tech-stack.md]

**Asynchronous Processing**: Utilize AWS SQS for background job processing as defined in architecture. Worker processes dequeue simulation jobs and execute browser automation. [Source: architecture/tech-stack.md, architecture/high-level-architecture.md]

### File Locations
**Service Layer**: Create `src/services/simulation_service.py` for simulation runner logic following established service pattern. [Source: architecture/source-tree.md]

**API Layer**: Extend `src/api/simulations.py` with new simulation start endpoint. [Source: architecture/source-tree.md]

**Worker Processing**: Extend `src/worker.py` with simulation job handling logic. [Source: architecture/source-tree.md]

**Test Files**: Create `tests/unit/test_simulation_service.py` and extend `tests/unit/test_api_simulations.py` following test directory structure. [Source: architecture/coding-standards.md]

### Testing Requirements
**Test Strategy**: Use pytest with unittest.mock for unit tests. Mock Playwright browser automation for testing. Use moto for AWS service mocking in integration tests. Aim for 80% line coverage. [Source: architecture/test-strategy-and-standards.md]

**Test Organization**: Test files mirror src/ directory structure in tests/ directory. Use pytest fixtures for test data management. [Source: architecture/test-strategy-and-standards.md, architecture/coding-standards.md]

### Technical Constraints
**AWS Lambda Limitations**: Browser automation in Lambda may require containerized deployment or alternative execution environment. Consider execution time limits and memory constraints. Diff analysis and test script generation may require AI agent integration. [Source: architecture/tech-stack.md]

**Security Requirements**: Use structured logging (no print statements). Handle secrets securely. Raise custom exceptions for API error handling. Follow PEP 8 naming conventions. [Source: architecture/coding-standards.md]

**Service Layer Pattern**: All external I/O through designated service components. Separate concerns between API, service, and worker layers. [Source: architecture/coding-standards.md]

## Testing
**Test Framework**: pytest 8.0+ with unittest.mock for mocking external dependencies
**Test Location**: tests/unit/ and tests/integration/ directories mirroring src/ structure
**Coverage Goal**: 80% line coverage enforced by CI/CD pipeline
**AWS Mocking**: Use moto library for mocking AWS services (SQS, DynamoDB) in tests
**Browser Mocking**: Mock Playwright browser automation for unit tests to avoid actual browser launches
**Test Data**: Use pytest fixtures for managing test data and setup/teardown

## Change Log
| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| 2025-09-20 | 1.0 | Initial story draft created | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be populated by development agent)

### Debug Log References
(To be populated by development agent)

### Completion Notes
(To be populated by development agent)

### File List
(To be populated by development agent)

## QA Results
(To be populated by QA agent)
